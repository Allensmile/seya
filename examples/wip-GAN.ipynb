{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This code needs to be updated to Keras' new API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember how we generate random values starting from a uniformely distributed random variable $U$? We can do that using the inverse transform techinique, which states that $X=F^{-1}(U)$ is random variable with [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (CDF) given by $F_X(X) = F$.\n",
    "\n",
    "For example, assume we want to generate a exponentialy distributed random variable. Wikipedia gives us the following facts:\n",
    "1. Probability disbutition function (PDF):\n",
    "\\begin{equation}\n",
    "f_X(x) = \\lambda e^{-\\lambda x}\n",
    "\\end{equation}\n",
    "2. CDF:\n",
    "\\begin{equation}\n",
    "F_X(x) = \\int_{-inf}^x f_X(z)dz = 1-e^{-\\lambda x}\n",
    "\\end{equation}\n",
    "3. Inverting that CDF we have:\n",
    "\\begin{equation}\n",
    "F_X^{-1}(u) = -\\frac{1}{\\lambda}log(1-u)\n",
    "\\end{equation}\n",
    "\n",
    "Let's check that with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "l = 1 # lambda\n",
    "u = np.random.uniform(0, 1, (500,))\n",
    "e = -np.log(1-u) / l\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(u)\n",
    "plt.title('uniform')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(e)\n",
    "_ = plt.title('exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! But what if we want to generate pictures like those in the MNIST dataset? Can we estimate something like and inverse CDF in this case? Well, it happens that this is a hard problem and it has the field to generative modeling inside machine learning entirely dedicated to solve problems like those. If you remember Restricted Boltzman Machines (RBM), Autoencoders (AE), Sparse Coding, etc, that's what I'm talking about. Here we will be interested in a solution proposed by Goodfellow et. al [1] called Generative Adversarial Networks (GAN).\n",
    "\n",
    "GANs propose to generate samples from a dataset using (guess what?) adaptive neural networks. Thus, given a uniformely distributed vector $\\mathbf{U}$, the generated samples are given by $\\mathbf{X} = G(\\mathbf{U})$. Which doesn't quite solve the problem before we define how to train the DNN $G(\\cdot)$. Let us think about that next, following Goodfellow et. al [1].\n",
    "\n",
    "Given a dataset $S$, and a test statistic $D(\\cdot)$, we say that a sample $\\mathbf{X}$ comes from the underlying PDF generating the sample in $S$ if $D(\\mathbf{X}) = 1$. We reject that hypothesis if $D(\\mathbf{X}) = 0$. I would give you a dollar if you can guess how the guys from Bengio's lab proposed to parameterize $D(\\cdot)$... \n",
    "\n",
    "WRONG! They used DNNs. \n",
    "\n",
    "As a note, their derivations are general enough, and we can use other adaptive models instead. That being said, if $G$ wants to be good enough it must pass the test defined by $D$. On the other hand, $D$ must play it hard and only pass the real samples coming from $S$ itself. This way one can define a min-max game where the cost function for $G$ is the binary crossentropy between $D(G(\\mathbf{U}))$ and 1, i.e\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_G = log(D(G(\\mathbf{U}))),\n",
    "\\end{equation}\n",
    "\n",
    "the cost function for $D$, on the other hand, is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_D = \\begin{cases}\n",
    "        \\begin{array}{lcl}\n",
    "        log(D(\\mathbf{X})), \\quad\\quad if: \\mathbf{X}\\in S \\\\\n",
    "        log(1 - D(\\mathbf{X})), \\quad if: \\mathbf{X}=G(\\mathbf{U}).\n",
    "        \\end{array}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the final solution (minimum for the combined cost functions above) should give us realistic looking samples $G(\\mathbf{U})$ and a totally fooled $D$ returning $D(X) = \\frac{1}{2}$. Hopefully we understood that well enough to hack something with Keras. Let us do that for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This first line are a copy paste from keras/examples/mnist_mlp.py\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Reshape, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.advanced_activations import LeakyReLU as lrelu\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "from seya.layers.base import Unpool\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our model for $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 2046\n",
    "\n",
    "detector = Sequential()\n",
    "detector.add(Dense(784, dim))\n",
    "detector.add(lrelu())\n",
    "detector.add(Dropout(.3))\n",
    "detector.add(Dense(dim, dim))\n",
    "detector.add(Activation('tanh'))\n",
    "detector.add(Dropout(.3))\n",
    "detector.add(Dense(dim, 1)) # 1: Yes, it belongs to S, 0: fake!\n",
    "detector.add(Activation('sigmoid'))\n",
    "\n",
    "opt_d = Adam(lr=.002)\n",
    "detector.compile(loss='binary_crossentropy', optimizer=opt_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detector = Sequential()\n",
    "\n",
    "# Convolution2D parameters are\n",
    "#   (num_channels_out, num_channels_in, height, width)\n",
    "#   I got these from keras examples\n",
    "detector.add(Convolution2D(32, 1, 3, 3))\n",
    "detector.add(Activation('relu'))\n",
    "detector.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "detector.add(Dropout(0.25))\n",
    "detector.add(Convolution2D(32, 32, 3, 3))\n",
    "detector.add(Activation('relu'))\n",
    "detector.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "detector.add(Dropout(0.25))\n",
    "\n",
    "detector.add(Flatten())\n",
    "detector.add(Dense(800, 1))\n",
    "detector.add(Activation('sigmoid'))\n",
    "\n",
    "opt_d = Adam(lr=.002) \n",
    "detector.compile(loss='binary_crossentropy', optimizer=opt_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "detector.predict(np.ones((3, 1, 28, 28))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a copy of $D$ that we should use to adapt the cost function for $G$. Note that this copy should not be adapted with gradients coming from $\\mathcal{L}_G$, otherwise, $D$ will be cheating and we won't learn anything useful. To do that with Keras, we simply erase its params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detector_no_grad = copy(detector)\n",
    "detector_no_grad.params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can define $G$ using our hacked detector above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = Sequential()\n",
    "sampler.add(Dense(1600, 1600))\n",
    "sampler.add(Activation('relu'))\n",
    "sampler.add(Reshape(1, 40, 40))\n",
    "sampler.add(Convolution2D(32, 1, 7, 7, border_mode='valid'))\n",
    "sampler.add(Activation('relu'))\n",
    "sampler.add(Convolution2D(1, 32, 7, 7, border_mode='valid'))\n",
    "sampler.add(Activation('sigmoid'))\n",
    "\n",
    "sample_fake = theano.function([sampler.get_input()],\n",
    "                              sampler.get_output(), allow_input_downcast=True) # this is G itself\n",
    "# Debugging\n",
    "# I tested width and heights above till the following function\n",
    "#  printed (3, 1, 28, 28)\n",
    "#  28x28 is the size of MNIST images\n",
    "print( sample_fake(np.ones((3, 1600))).shape ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.add(detector_no_grad)\n",
    "\n",
    "opt_g = Adam(lr=.001)\n",
    "sampler.compile(loss='binary_crossentropy', optimizer=opt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FC model\n",
    "\n",
    "sampler = Sequential()\n",
    "sampler.add(Dense(dim, dim))\n",
    "sampler.add(lrelu())\n",
    "sampler.add(Dense(dim, dim))\n",
    "sampler.add(lrelu())\n",
    "sampler.add(Dense(dim, 784))\n",
    "sampler.add(Activation('sigmoid'))\n",
    "\n",
    "sample_fake = theano.function([sampler.get_input()], sampler.get_output()) # this is G itself\n",
    "\n",
    "sampler.add(detector_no_grad)\n",
    "\n",
    "opt_g = Adam(lr=.001) # I got better results when \n",
    "                      # detector's learning rate is faster\n",
    "sampler.compile(loss='binary_crossentropy', optimizer=opt_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can train our networks. First we sample a uniform random vector $\\mathbf{U}$, using that we sample fake images with $G(\\mathbf{U})$. We train $G$ to get passing values in the $D$ test, I mean 1. Later, we concatenate the fake images with some real MNIST images and train $D$ to return zeros for the formers and ones for the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12\n",
      "----------------------------------------\n",
      "Training...\n",
      "----------------------------------------\n",
      "Epoch 13\n",
      "----------------------------------------\n",
      "Training...\n",
      "----------------------------------------\n",
      "Epoch 14\n",
      "----------------------------------------\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9c54795817c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m                              range(batch_size, X_train.shape[0], batch_size)):\n\u001b[0;32m     18\u001b[0m         \u001b[0mnoise_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mfake_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_fake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         true_n_fake = np.concatenate([X_train[first: last].reshape((-1, 1, 28, 28)),\n\u001b[0;32m     21\u001b[0m                                       fake_samples], axis=0)\n",
      "\u001b[1;32m/home/eders/python/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eders/python/Theano/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoContext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafc5d5b350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_epoch = 1000 # it takes some time to get something recognizable.\n",
    "batch_size = 128\n",
    "num_batches = X_train.shape[0] / 128\n",
    "fig = plt.figure()\n",
    "fixed_noise = np.random.uniform(-1, 1, (9, 1600)).astype('float32') # Let us visualize how these sampes evolve with training\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    print('-'*40)\n",
    "    print('Epoch', e)\n",
    "    print('-'*40)\n",
    "    print(\"Training...\")\n",
    "    # batch train with realtime data augmentation\n",
    "    #progbar = generic_utils.Progbar(X_train.shape[0])\n",
    "    loss0 = 0\n",
    "    loss1 = 1\n",
    "    for (first, last) in zip(range(0, X_train.shape[0]-batch_size, batch_size),\n",
    "                             range(batch_size, X_train.shape[0], batch_size)):\n",
    "        noise_batch = np.random.uniform(-1, 1, (batch_size, 1600)).astype('float32')\n",
    "        fake_samples = sample_fake(noise_batch)\n",
    "        true_n_fake = np.concatenate([X_train[first: last].reshape((-1, 1, 28, 28)),\n",
    "                                      fake_samples], axis=0)\n",
    "        y_batch = np.concatenate([np.ones((batch_size, 1)),\n",
    "                                  np.zeros((batch_size, 1))], axis=0).astype('float32')\n",
    "        all_fake = np.ones((batch_size, 1)).astype('float32')\n",
    "        # We take turns adapting G and D. We may give D an upper hand,\n",
    "        #  letting it train for more turns, keeping G fixed.\n",
    "        #  Do that increasing the upper hand `uh` variable.\n",
    "        uh = 2\n",
    "        if e % uh == 0:\n",
    "            loss0 += sampler.train_on_batch(noise_batch, all_fake)\n",
    "        else:\n",
    "            loss1 += detector.train_on_batch(true_n_fake, y_batch)\n",
    "        #loss = loss0 + loss1\n",
    "        #progbar.add(batch_size, values=[(\"train loss\", loss)])\n",
    "    #print('Gerative loss {0}'.format(loss0))\n",
    "    #print('Discriminative loss {0}'.format(loss1))\n",
    "    #print('Total loss {0}'.format(((loss0+loss1))))\n",
    "    \n",
    "    # Uncoment the lines above if you feel like\n",
    "    \n",
    "    if e % 10 == 0: # visualize results once in a while\n",
    "        fixed_fake = sample_fake(fixed_noise)\n",
    "        plt.clf()\n",
    "        for i in range(9):\n",
    "            plt.subplot(3, 3, i+1)\n",
    "            plt.imshow(fixed_fake[i].reshape((28,28)), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        plt.show()\n",
    "        plt.savefig(str(e)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
